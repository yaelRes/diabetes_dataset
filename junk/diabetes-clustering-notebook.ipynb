{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Diabetes Dataset Clustering Analysis\n",
    "\n",
    "This notebook performs comprehensive clustering analysis on a diabetes dataset using various dimensionality reduction techniques and clustering algorithms to identify patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# ! pip install scikit-learn hdbscan umap",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import hdbscan\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "# Modify the path as needed to point to your file\n",
    "data = pd.read_csv(\"diabetes_dataset.csv\")\n",
    "   # None was convert to nan\n",
    "data['Alcohol_Consumption'] = data['Alcohol_Consumption'].fillna('None')\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check dataset info\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(data.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check data types and basic statistics\n",
    "data.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic statistics for numerical columns\n",
    "data.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check distribution of categorical variables\n",
    "categorical_cols = ['Sex', 'Ethnicity', 'Physical_Activity_Level', \n",
    "                   'Alcohol_Consumption', 'Smoking_Status']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nDistribution of {col}:\")\n",
    "    print(data[col].value_counts())\n",
    "    print(f\"Percentage:\\n{data[col].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# Specific check for Alcohol_Consumption to ensure 'None' is properly recognized\n",
    "print(\"\\nSpecific check for Alcohol_Consumption values:\")\n",
    "alcohol_values = data['Alcohol_Consumption'].unique()\n",
    "print(alcohol_values)\n",
    "print(\"\\nVerifying 'None' is treated as a valid category (not missing):\")\n",
    "print(f\"Count of 'None' values: {(data['Alcohol_Consumption'] == 'None').sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "# Drop the first unnamed index column if present\n",
    "if '' in data.columns:\n",
    "    data = data.drop('', axis=1)\n",
    "elif 'Unnamed: 0' in data.columns:\n",
    "    data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "categorical_cols = ['Sex', 'Ethnicity', 'Physical_Activity_Level', \n",
    "                   'Alcohol_Consumption', 'Smoking_Status']\n",
    "numerical_cols = [col for col in data.columns if col not in categorical_cols and \n",
    "                 col not in ['Family_History_of_Diabetes', 'Previous_Gestational_Diabetes']]\n",
    "binary_cols = ['Family_History_of_Diabetes', 'Previous_Gestational_Diabetes']\n",
    "\n",
    "print(\"Numerical features:\", numerical_cols)\n",
    "print(\"\\nCategorical features:\", categorical_cols)\n",
    "print(\"\\nBinary features:\", binary_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define preprocessing for numerical and categorical data\n",
    "# Ensure OneHotEncoder handles 'None' as a valid category in Alcohol_Consumption\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols+binary_cols)\n",
    "    ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numerical_cols),\n",
    "#         ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "#         ('bin', 'passthrough', binary_cols)\n",
    "#     ])\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessing_pipeline.fit_transform(data)\n",
    "\n",
    "print(f\"Shape of processed data: {X_processed.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Utility Functions for Clustering and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_clustering(X, labels, method_name):\n",
    "    \"\"\"Evaluate clustering performance using silhouette score\"\"\"\n",
    "    if len(np.unique(labels)) > 1:  # Ensure we have more than one cluster\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        print(f\"{method_name} - Silhouette Score: {silhouette:.4f}\")\n",
    "        return silhouette\n",
    "    else:\n",
    "        print(f\"{method_name} - Only one cluster found, cannot calculate silhouette score\")\n",
    "        return -1\n",
    "\n",
    "def plot_clusters_2d(X_2d, labels, method_name, title):\n",
    "    \"\"\"Plot clusters in 2D space\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='viridis', \n",
    "                         alpha=0.7, s=40, edgecolors='w', linewidth=0.5)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title(f\"{title}\\n{method_name}\", fontsize=16)\n",
    "    plt.xlabel('Dimension 1', fontsize=12)\n",
    "    plt.ylabel('Dimension 2', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def run_dimensionality_reduction(X, methods):\n",
    "    \"\"\"Apply various dimensionality reduction methods\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method in methods.items():\n",
    "        print(f\"Applying {method_name}...\")\n",
    "        X_reduced = method.fit_transform(X)\n",
    "        results[method_name] = X_reduced\n",
    "        \n",
    "    return results\n",
    "\n",
    "def run_clustering(X, methods):\n",
    "    \"\"\"Apply various clustering methods\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method in methods.items():\n",
    "        print(f\"Applying {method_name}...\")\n",
    "        labels = method.fit_predict(X)\n",
    "        silhouette = evaluate_clustering(X, labels, method_name)\n",
    "        results[method_name] = {\n",
    "            'labels': labels,\n",
    "            'silhouette': silhouette,\n",
    "            'n_clusters': len(np.unique(labels))\n",
    "        }\n",
    "        \n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define dimensionality reduction methods\n",
    "dim_reduction_methods = {\n",
    "    'PCA n_components=2': PCA(n_components=2),\n",
    "    'UMAP n_components=2, n_neighbors=15, min_dist=0.1': umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42),\n",
    "    't-SNE n_components=2, perplexity=30, n_iter=1000': TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Run dimensionality reduction\n",
    "reduced_data = run_dimensionality_reduction(X_processed, dim_reduction_methods)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:16.702165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a side-by-side comparison of all dimensionality reduction techniques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "reduction_methods = list(reduced_data.keys())\n",
    "\n",
    "for i, (dr_name, X_reduced) in enumerate(reduced_data.items()):\n",
    "    scatter = axes[i].scatter(\n",
    "        X_reduced[:, 0], X_reduced[:, 1],\n",
    "        alpha=0.6, s=0.5)\n",
    "    axes[i].set_title(f\"{dr_name}\", fontsize=16)\n",
    "    axes[i].set_xlabel('X', fontsize=14)\n",
    "    axes[i].set_ylabel('Y', fontsize=14)\n",
    "    \n",
    "# Add a single colorbar for all plots\n",
    "cbar = fig.colorbar(scatter, ax=axes, label='Cluster')\n",
    "plt.suptitle(f\"Comparison of Dimensionality Reduction Techniques \", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()\n",
    "\n",
    "# Save the comparison image if needed\n",
    "# plt.savefig('dim_reduction_comparison.png', dpi=300, bbox_inches='tight')\n",
    "# print(\"Comparison image saved as 'dim_reduction_comparison.png'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:17.317769Z"
    }
   },
   "source": [
    "# Define clustering methods\n",
    "clustering_methods = {\n",
    "    'Agglomerative (k=5)': AgglomerativeClustering(n_clusters=5),\n",
    "    'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=50, min_samples=5)\n",
    "}\n",
    "for k in range(2,6):\n",
    "    clustering_methods[f'K-Means (k={k})']= KMeans(n_clusters=k, random_state=42)\n",
    "    \n",
    "# Run clustering on the original high-dimensional data\n",
    "clustering_results = run_clustering(X_processed, clustering_methods)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:31.049874Z"
    }
   },
   "source": [
    "# Find the best clustering method based on silhouette score\n",
    "best_method = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])[0]\n",
    "best_labels = clustering_results[best_method]['labels']\n",
    "print(f\"\\nBest clustering method: {best_method} with silhouette score: {clustering_results[best_method]['silhouette']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Clusters with Different Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:31.065995Z"
    }
   },
   "source": [
    "# Visualize the best clustering method with different dimensionality reduction techniques\n",
    "for dr_name, X_reduced in reduced_data.items():\n",
    "    plt = plot_clusters_2d(\n",
    "        X_reduced, \n",
    "        best_labels, \n",
    "        f\"{best_method}\", \n",
    "        f\"Diabetes Dataset Clusters Visualized with {dr_name}\"\n",
    "    )\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:32.438742Z"
    }
   },
   "source": [
    "# Add cluster labels to the original data\n",
    "data['Cluster'] = best_labels\n",
    "\n",
    "# Analyze the characteristics of each cluster\n",
    "cluster_summary = data.groupby('Cluster').mean()\n",
    "print(\"Cluster Characteristics (mean values):\")\n",
    "cluster_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:32.599173Z"
    }
   },
   "source": [
    "# Function to visualize feature distributions by cluster\n",
    "def plot_feature_distributions(data, feature_list, n_cols=3):\n",
    "    n_features = len(feature_list)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(feature_list):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(x='Cluster', y=feature, data=data, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {feature} by Cluster')\n",
    "            \n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    return plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:32.601186Z"
    }
   },
   "source": [
    "# Select the most important features for visualization\n",
    "important_features = [\n",
    "    'Age', 'BMI', 'Fasting_Blood_Glucose', 'HbA1c', \n",
    "    'Waist_Circumference', 'Blood_Pressure_Systolic',\n",
    "    'Cholesterol_Total', 'Cholesterol_HDL', 'Cholesterol_LDL'\n",
    "]\n",
    "\n",
    "# Plot feature distributions by cluster\n",
    "plt = plot_feature_distributions(data, important_features)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis using PCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:32.602246Z"
    }
   },
   "source": [
    "# Create a PCA-based feature importance plot\n",
    "pca = PCA()\n",
    "pca.fit(X_processed)\n",
    "\n",
    "# Create a dataframe with PCA components and their explained variance\n",
    "pca_df = pd.DataFrame({\n",
    "    'Variance Explained': pca.explained_variance_ratio_,\n",
    "    'Principal Component': [f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))]\n",
    "})\n",
    "\n",
    "# Plot the explained variance of PCA components\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Principal Component', y='Variance Explained', data=pca_df.head(10))\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Cluster Centers (for K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:32.603199Z"
    }
   },
   "source": [
    "# Generate a heatmap of cluster centers for the most important features\n",
    "# First, we need to get the cluster centers\n",
    "if 'K-Means' in best_method:\n",
    "    # For K-Means, we can directly get the cluster centers\n",
    "    kmeans = clustering_methods[best_method]\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Process feature names for the heatmap\n",
    "    feature_names = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name == 'num':\n",
    "            feature_names.extend(cols)\n",
    "        elif name == 'cat':\n",
    "            # Get one-hot encoded column names\n",
    "            encoder = trans\n",
    "            for i, col in enumerate(cols):\n",
    "                cats = encoder.categories_[i][1:]  # Skip the first category (dropped)\n",
    "                feature_names.extend([f\"{col}_{cat}\" for cat in cats])\n",
    "        elif name == 'bin':\n",
    "            feature_names.extend(cols)\n",
    "    \n",
    "    # Create a DataFrame with cluster centers\n",
    "    centers_df = pd.DataFrame(cluster_centers[:, :len(numerical_cols)], \n",
    "                             columns=numerical_cols)\n",
    "    \n",
    "    # Normalize the centers for heatmap visualization\n",
    "    centers_norm = (centers_df - centers_df.mean()) / centers_df.std()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(centers_norm, annot=True, cmap='coolwarm', linewidths=.5,\n",
    "               yticklabels=[f'Cluster {i}' for i in range(centers_norm.shape[0])])\n",
    "    plt.title('Normalized Cluster Centers for Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Distribution of Categorical Variables by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:37:32.642447Z",
     "start_time": "2025-04-24T21:37:32.605192Z"
    }
   },
   "source": [
    "# Analyze the distribution of categorical variables across clusters\n",
    "for cat_col in categorical_cols:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    cross_tab = pd.crosstab(data['Cluster'], data[cat_col], normalize='index') * 100\n",
    "    cross_tab.plot(kind='bar', stacked=True)\n",
    "    plt.title(f'Distribution of {cat_col} by Cluster (%)')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title=cat_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary of Findings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-24T21:37:32.606257Z"
    }
   },
   "source": [
    "# Display a summary of the clustering results\n",
    "print(f\"Best clustering method: {best_method}\")\n",
    "print(f\"Number of clusters: {clustering_results[best_method]['n_clusters']}\")\n",
    "print(f\"Silhouette score: {clustering_results[best_method]['silhouette']:.4f}\")\n",
    "\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(data['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nKey characteristics of each cluster:\")\n",
    "# Display the top differentiating features for each cluster\n",
    "for cluster in sorted(data['Cluster'].unique()):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    # Calculate the z-scores for this cluster compared to overall mean\n",
    "    cluster_means = data[data['Cluster'] == cluster][numerical_cols].mean()\n",
    "    overall_means = data[numerical_cols].mean()\n",
    "    overall_stds = data[numerical_cols].std()\n",
    "    \n",
    "    z_scores = (cluster_means - overall_means) / overall_stds\n",
    "    \n",
    "    # Display top 5 features with highest absolute z-scores\n",
    "    top_features = z_scores.abs().sort_values(ascending=False).head(5).index.tolist()\n",
    "    for feature in top_features:\n",
    "        direction = \"higher\" if z_scores[feature] > 0 else \"lower\"\n",
    "        print(f\"  - {feature}: {direction} than average (z-score: {z_scores[feature]:.2f})\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
